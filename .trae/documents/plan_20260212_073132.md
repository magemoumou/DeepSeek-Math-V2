## 基于DeepSeekMath-V2的IMO数学竞赛解题指南

### 🎯 核心发现：专用模板vs普通LLM的本质区别

通过深入分析DeepSeekMath-V2的源码，我发现它与普通LLM的最大区别在于：**三重验证机制和结构化模板设计**。

### 📋 最优Prompt工程实践

#### 1. 证明生成模板（proof_generation）
**核心要素**：
- **自我评估要求**: 强制模型在给出解答后进行自我评分
- **评分标准**: 0、0.5、1三级评分体系
- **格式约束**: 必须包含"## Solution"和"## Self Evaluation"部分
- **诚实要求**: 明确要求模型不能"作弊"，必须诚实指出错误

**关键prompt结构**:
```
Your task is to solve a given problem... 
Your final solution should be exceptionally comprehensive and easy-to-follow...
In fact, you already have the ability to rate your solution yourself...
Remember! You CAN'T cheat! If you cheat, we will know, and you will be penalized!
```

#### 2. 证明验证模板（proof_verification）
**验证维度**:
- **完整性检查**: 所有步骤是否正确执行
- **细节评估**: 是否有细节遗漏或轻微错误  
- **致命错误识别**: 是否包含根本性错误
- **引用验证**: 对文献引用的合理性检查

#### 3. 元验证模板（meta_verification）
**二次验证**: 验证评估本身的合理性，防止误判

### 🔄 完整解题Workflow设计

#### 第一阶段：初始证明生成
1. 使用`proof_generation`模板生成初步解答
2. 强制模型进行自我评估和评分
3. 要求诚实指出所有潜在问题

#### 第二阶段：多轮迭代优化
1. **证明验证**: 使用`proof_verification`模板评估证明质量
2. **元验证**: 使用`meta_verification`验证评估合理性
3. **证明精炼**: 基于反馈使用`proof_refinement`模板改进
4. **循环迭代**: 重复1-3步直到达到满意质量

#### 第三阶段：最终验证
1. 多维度验证确保证明严谨性
2. 交叉验证防止单一视角偏差
3. 质量评分确保达到竞赛标准

### 💡 与普通LLM的关键差异

| 维度 | 普通LLM | DeepSeekMath-V2 |
|------|---------|-----------------|
| **验证机制** | 无验证 | 三重验证 |
| **评分体系** | 无评分 | 0-0.5-1三级评分 |
| **自我纠错** | 依赖提示 | 强制自我评估 |
| **迭代优化** | 手动优化 | 自动多轮迭代 |
| **诚实约束** | 无约束 | 强制诚实声明 |

### 🚀 实际使用建议

#### 1. 单题解答模式
```python
# 直接使用proof_generation模板
question = "IMO数学题目"
prompt = math_templates["proof_generation"].format(question=question)
```

#### 2. 高质量解答模式
```python
# 完整workflow：生成→验证→元验证→精炼
# 推荐迭代3-5轮获得最佳结果
```

#### 3. 参数调优建议
- **温度参数**: 证明生成1.0，验证阶段1.0（保持创造性）
- **最大token**: 证明生成128K，验证64K（确保充分展开）
- **并行度**: 根据硬件条件调整，建议32-128之间

### 🎓 教育意义

这套体系不仅适用于IMO解题，更重要的是展示了AI如何通过**自我反思和迭代改进**来提升推理质量，这对教育领域有重要启发意义。