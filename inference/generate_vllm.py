import os
import json
import pickle
import math
import argparse
import asyncio
import aiohttp

from tqdm import tqdm
from multiprocessing import Queue, Process
from time import time, sleep

# vLLMå…¼å®¹çš„APIModelç±»ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰
class APIModel:
    def __init__(self):
        """
        åˆå§‹åŒ–APIModelï¼Œæ”¯æŒvLLMæœ¬åœ°æ¨ç†å¼•æ“
        é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼š
        - VLLM_API_KEY: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º"EMPTY"ï¼‰
        - VLLM_BASE_URL: vLLMæ¨ç†å¼•æ“åœ°å€ï¼ˆå¦‚ï¼šhttp://localhost:8000/v1ï¼‰
        """
        self.api_key = os.getenv("VLLM_API_KEY", "EMPTY")
        self.base_url = os.getenv("VLLM_BASE_URL", "http://localhost:8000/v1")
        self.timeout = 300  # 5åˆ†é’Ÿè¶…æ—¶
        
        # éªŒè¯é…ç½®
        if not self.base_url:
            raise ValueError("å¿…é¡»è®¾ç½®VLLM_BASE_URLç¯å¢ƒå˜é‡æˆ–é€šè¿‡ä»£ç é…ç½®base_url")
        
        print(f"ğŸ”§ åˆå§‹åŒ–vLLMå®¢æˆ·ç«¯:")
        print(f"   Base URL: {self.base_url}")
        print(f"   API Key: {self.api_key[:10]}..." if len(self.api_key) > 10 else f"   API Key: {self.api_key}")

    async def generate_one(self, prompt, sampling_params):
        """
        å•æ¬¡ç”Ÿæˆè°ƒç”¨ï¼Œå…¼å®¹vLLM APIæ ¼å¼
        
        Args:
            prompt: æ¶ˆæ¯åˆ—è¡¨æ ¼å¼çš„prompt
            sampling_params: é‡‡æ ·å‚æ•°å­—å…¸
            
        Returns:
            (output_string, finish_reason) å…ƒç»„
        """
        try:
            # æ„å»ºvLLMå…¼å®¹çš„è¯·æ±‚ä½“
            request_body = {
                "model": sampling_params.get("model", "default"),
                "messages": prompt,
                "temperature": sampling_params.get("temperature", 1.0),
                "top_p": sampling_params.get("top_p", 0.95),
                "max_tokens": sampling_params.get("max_tokens", 128000),
                "stream": False,
            }
            
            # æ¸…ç†Noneå€¼
            request_body = {k: v for k, v in request_body.items() if v is not None}
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout)) as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=request_body
                ) as response:
                    
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"vLLM APIé”™è¯¯: {response.status} - {error_text}")
                    
                    result = await response.json()
                    
                    # å¤„ç†vLLMå“åº”æ ¼å¼
                    choice = result["choices"][0]
                    message = choice["message"]
                    
                    # æå–æ¨ç†å†…å®¹å’Œä¸»è¦å†…å®¹
                    reasoning_content = message.get("reasoning_content", "").strip()
                    content = message.get("content", "").strip()
                    
                    # æ„å»ºä¸åŸå§‹æ ¼å¼å…¼å®¹çš„è¾“å‡º
                    output_string = f"<think>\n{reasoning_content}"
                    if content:
                        output_string = reasoning_content + f"\n</think>\n{content}"
                    
                    finish_reason = choice.get("finish_reason", "stop").lower()
                    
                    return output_string.strip(), finish_reason
                    
        except Exception as e:
            print(f"âŒ ç”Ÿæˆé”™è¯¯: {e}")
            # è¿”å›é”™è¯¯æ—¶çš„é»˜è®¤å€¼
            return f"<think>\næ¨ç†é”™è¯¯: {str(e)}\n</think>\n", "error"

    async def generate_all(self, data):
        """æ‰¹é‡å¼‚æ­¥ç”Ÿæˆ"""
        tasks = [self.generate_one(task['prompt'], task['sampling_params']) for task in data]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ ä»»åŠ¡ {i} å¤±è´¥: {result}")
                processed_results.append((f"<think>\né”™è¯¯: {str(result)}\n</think>\n", "error"))
            else:
                processed_results.append(result)
        
        return processed_results

    def generate(self, input_data, sampling_params):
        """åŒæ­¥ç”Ÿæˆæ¥å£"""
        data = []
        for item in input_data:
            if "messages" not in item:
                messages = [{
                    "role": "user",
                    "content": item["prompt"],
                }]
            else:
                messages = item['messages']
            
            data.append({
                'prompt': messages,
                'sampling_params': sampling_params
            })

        outputs = asyncio.run(self.generate_all(data))
        output_data = []
        
        assert len(input_data) == len(outputs), f"è¾“å…¥è¾“å‡ºæ•°é‡ä¸åŒ¹é…: {len(input_data)} vs {len(outputs)}"
        
        for item, (output_string, finish_reason) in zip(input_data, outputs):
            output_data.append({
                **item,
                "output": output_string,
                "finish_reason": finish_reason,
            })
        
        return output_data

    def mp_generate(self, input_queue: Queue, output_queue: Queue, sampling_params):
        """å¤šè¿›ç¨‹ç”Ÿæˆæ¥å£"""
        while True:
            batch_idx, input_data = input_queue.get()
            if input_data is None:
                output_queue.put((batch_idx, None))
                break
            
            try:
                output_data = self.generate(input_data, sampling_params)
                output_queue.put((batch_idx, output_data))
            except Exception as e:
                print(f"âŒ æ‰¹é‡ç”Ÿæˆé”™è¯¯: {e}")
                error_output = [{
                    **item,
                    "output": f"<think>\næ‰¹é‡ç”Ÿæˆé”™è¯¯: {str(e)}\n</think>\n",
                    "finish_reason": "error",
                } for item in input_data]
                output_queue.put((batch_idx, error_output))


def mp_generate_loop(input_queue, output_queue, sampling_params):
    """å¤šè¿›ç¨‹ç”Ÿæˆå¾ªç¯"""
    api_model = APIModel()
    sleep(5)  # åˆå§‹åŒ–å»¶è¿Ÿ
    api_model.mp_generate(input_queue, output_queue, sampling_params)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_data_path", required=True)
    parser.add_argument("--output_data_path", required=True)
    parser.add_argument("--num_processes", default=16, type=int)
    parser.add_argument("--batch_size", default=16, type=int)
    parser.add_argument("--temperature", required=True, type=float)
    parser.add_argument("--top_p", required=True, type=float)
    parser.add_argument("--max_tokens", required=True, type=int)
    parser.add_argument("--n", required=True, type=int)
    
    args, _ = parser.parse_known_args()
    
    input_data_path, output_data_path = args.input_data_path, args.output_data_path
    os.makedirs(os.path.dirname(output_data_path), exist_ok=True)

    num_processes = args.num_processes
    batch_size = args.batch_size
    temperature = args.temperature
    top_p = args.top_p
    max_tokens = args.max_tokens
    n = args.n

    # å…ƒæ•°æ®å¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    meta_data_path = f"{output_data_path}.meta"
    if not os.path.exists(meta_data_path):
        meta_data = {"n": n, "batch_size": batch_size, "complete_batches": []}
        with open(meta_data_path, "wb") as f:
            pickle.dump(meta_data, f)
    with open(meta_data_path, "rb") as f:
        meta_data = pickle.load(f)
    meta_data["complete_batches"] = set(meta_data["complete_batches"])

    assert n == meta_data["n"] and batch_size == meta_data["batch_size"], \
        f"params n or batch_size are different from previous running setting({n}, {batch_size}) != ({meta_data['n']}, {meta_data['batch_size']}), you need to delete {output_data_path} & {meta_data_path} to clear existing results"

    sampling_params = dict(
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
        model="default"  # vLLMéœ€è¦æŒ‡å®šæ¨¡å‹åç§°
    )

    input_queue, output_queue = Queue(), Queue()
    fr = open(input_data_path, "r", encoding="utf-8")
    fw = open(output_data_path, "a+", encoding="utf-8")

    processes = []
    
    for i in range(num_processes):
        process = Process(target=mp_generate_loop, args=(input_queue, output_queue, sampling_params))
        process.start()
        processes.append(process)

    submit_batch = []
    num_input = 0
    num_skip = 0
    batch_idx = 0

    for line in tqdm(fr, desc="Waiting Input"):
        item = json.loads(line)
        for i in range(n):
            submit_batch.append(item)
            if len(submit_batch) >= batch_size:
                if batch_idx not in meta_data["complete_batches"]:
                    num_input += batch_size
                    input_queue.put((batch_idx, submit_batch))
                else:
                    num_skip += batch_size
                batch_idx += 1
                submit_batch = []
    if len(submit_batch) > 0:
        if batch_idx not in meta_data["complete_batches"]:
            input_queue.put((batch_idx, submit_batch))
            num_input += len(submit_batch)
        else:
            num_skip += len(submit_batch)
    print(f"Total Input Samples: {num_input} (Skip {num_skip} Samples)")
    fr.close()

    for i in range(num_processes):
        input_queue.put((None, None))

    remain_processes = num_processes
    num_output = 0
    with tqdm(desc="Waiting Output", total=num_input) as pbar:
        while remain_processes > 0:
            batch_idx, output_data = output_queue.get()
            if output_data is None:
                remain_processes -= 1
                continue
            for item in output_data:
                print(json.dumps(item, ensure_ascii=False), file=fw, flush=True)
                num_output += 1
                pbar.update(1)
            meta_data["complete_batches"].add(batch_idx)
            with open(meta_data_path, "wb") as f:
                pickle.dump(meta_data, f)
            fw.flush()
    print(f"Total Output Samples: {num_output}")
    fw.close()
    [process.join() for process in processes]