# ğŸš€ DeepSeekMath-V2 vLLMå…¼å®¹æ€§é…ç½®æŒ‡å—

## ğŸ“‹ ç¯å¢ƒé…ç½®

### 1. ç¯å¢ƒå˜é‡é…ç½®ï¼ˆæ¨èæ–¹å¼ï¼‰
```bash
# è®¾ç½®vLLMæ¨ç†å¼•æ“åœ°å€
export VLLM_BASE_URL="http://localhost:8000/v1"

# è®¾ç½®APIå¯†é’¥ï¼ˆvLLMé€šå¸¸å¯ä»¥è®¾ç½®ä¸ºEMPTYï¼‰
export VLLM_API_KEY="EMPTY"

# å¯é€‰ï¼šè®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
export VLLM_TIMEOUT="300000"
```

### 2. ä»£ç ç›´æ¥é…ç½®ï¼ˆå¤‡é€‰æ–¹å¼ï¼‰
```python
# åœ¨ä»£ç ä¸­ç›´æ¥é…ç½®
api_model = APIModel(
    api_key="EMPTY",                    # vLLMé€šå¸¸ä¸éœ€è¦çœŸå®çš„APIå¯†é’¥
    base_url="http://localhost:8000/v1", # ä½ çš„vLLMæ¨ç†å¼•æ“åœ°å€
    timeout=300000                     # è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
)
```

## ğŸ”§ vLLMæ¨ç†å¼•æ“å¯åŠ¨ç¤ºä¾‹

### åŸºæœ¬å¯åŠ¨å‘½ä»¤
```bash
# å¯åŠ¨vLLMæ¨ç†æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype auto \
    --max-model-len 128000
```

### é«˜çº§é…ç½®ï¼ˆæ¨èï¼‰
```bash
# å¸¦GPUä¼˜åŒ–å’Œæ‰¹å¤„ç†çš„å¯åŠ¨å‘½ä»¤
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype float16 \
    --max-model-len 128000 \
    --gpu-memory-utilization 0.9 \
    --max-num-batched-tokens 8192 \
    --max-num-seqs 256 \
    --disable-log-stats
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€ä½¿ç”¨ï¼ˆç¯å¢ƒå˜é‡æ–¹å¼ï¼‰
```bash
# 1. è®¾ç½®ç¯å¢ƒå˜é‡
export VLLM_BASE_URL="http://localhost:8000/v1"
export VLLM_API_KEY="EMPTY"

# 2. è¿è¡Œæ¨ç†è„šæœ¬
python inference/generate.py \
    --input_data_path inputs/IMO2025.json \
    --output_data_path outputs/IMO2025_results.jsonl \
    --temperature 1.0 \
    --top_p 0.95 \
    --max_tokens 128000 \
    --n 32
```

### 2. å‘½ä»¤è¡Œå‚æ•°æ–¹å¼
```bash
# ç›´æ¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®švLLMé…ç½®
python inference/generate_vllm_compatible.py \
    --input_data_path inputs/IMO2025.json \
    --output_data_path outputs/IMO2025_results.jsonl \
    --temperature 1.0 \
    --top_p 0.95 \
    --max_tokens 128000 \
    --n 32 \
    --num_processes 16 \
    --batch_size 16
```

### 3. åœ¨Pythonä»£ç ä¸­ä½¿ç”¨
```python
from inference.generate_vllm_compatible import APIModel

# åˆå§‹åŒ–vLLMå®¢æˆ·ç«¯
api_model = APIModel()

# å‡†å¤‡æµ‹è¯•æ•°æ®
test_data = [{
    "prompt": "Prove that for any positive integer n, n^2 + n + 1 is always odd.",
    "problem_idx": "test-1"
}]

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = {
    "temperature": 1.0,
    "top_p": 0.95,
    "max_tokens": 4096,
    "model": "default"
}

# æ‰§è¡Œæ¨ç†
results = api_model.generate(test_data, sampling_params)

# è¾“å‡ºç»“æœ
for result in results:
    print(f"è¾“å…¥: {result['prompt']}")
    print(f"è¾“å‡º: {result['output']}")
    print(f"å®ŒæˆåŸå› : {result['finish_reason']}")
    print("-" * 50)
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜1ï¼šè¿æ¥è¶…æ—¶
```bash
# ç—‡çŠ¶ï¼šè¿æ¥vLLMæœåŠ¡è¶…æ—¶
# è§£å†³æ–¹æ¡ˆï¼š
1. æ£€æŸ¥vLLMæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ
   curl http://localhost:8000/v1/models

2. å¢åŠ è¶…æ—¶æ—¶é—´
   export VLLM_TIMEOUT="600000"  # 10åˆ†é’Ÿ

3. æ£€æŸ¥é˜²ç«å¢™å’Œç½‘ç»œé…ç½®
```

### å¸¸è§é—®é¢˜2ï¼šæ¨¡å‹åŠ è½½å¤±è´¥
```bash
# ç—‡çŠ¶ï¼švLLMå¯åŠ¨æ—¶æŠ¥CUDAå†…å­˜ä¸è¶³
# è§£å†³æ–¹æ¡ˆï¼š
1. å‡å°æ‰¹å¤„ç†å¤§å°
   --max-num-batched-tokens 4096
   --max-num-seqs 128

2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–é‡åŒ–
   --dtype float16
   --quantization awq

3. å‡å°‘GPUå†…å­˜åˆ©ç”¨ç‡
   --gpu-memory-utilization 0.8
```

### å¸¸è§é—®é¢˜3ï¼šæ¨ç†è´¨é‡å¼‚å¸¸
```python
# ç—‡çŠ¶ï¼šæ¨ç†ç»“æœè´¨é‡ä¸‹é™
# è§£å†³æ–¹æ¡ˆï¼š
# 1. è°ƒæ•´æ¸©åº¦å‚æ•°
sampling_params = {
    "temperature": 0.7,  # é™ä½æ¸©åº¦æé«˜ç¡®å®šæ€§
    "top_p": 0.9,       # é™ä½top_pæé«˜ä¸“æ³¨åº¦
    "max_tokens": 8192,  # é€‚å½“å¢åŠ æœ€å¤§tokenæ•°
}

# 2. æ£€æŸ¥promptæ¨¡æ¿æ˜¯å¦æ­£ç¡®åº”ç”¨
# 3. éªŒè¯æ¨¡å‹æƒé‡æ˜¯å¦å®Œæ•´åŠ è½½
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹å¤„ç†ä¼˜åŒ–
```bash
# æ ¹æ®GPUå†…å­˜è°ƒæ•´æ‰¹å¤„ç†å‚æ•°
--max-num-batched-tokens 16384  # å¢åŠ æ‰¹å¤„ç†tokenæ•°
--max-num-seqs 512              # å¢åŠ å¹¶å‘åºåˆ—æ•°
```

### 2. å¤šè¿›ç¨‹ä¼˜åŒ–
```bash
# æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´è¿›ç¨‹æ•°
python inference/generate_vllm_compatible.py \
    --num_processes 32 \        # å¢åŠ è¿›ç¨‹æ•°
    --batch_size 32 \          # å¢åŠ æ‰¹å¤§å°
    ...å…¶ä»–å‚æ•°
```

### 3. æ¨¡å‹å‚æ•°ä¼˜åŒ–
```python
# é’ˆå¯¹æ•°å­¦æ¨ç†ä¼˜åŒ–çš„é‡‡æ ·å‚æ•°
sampling_params = {
    "temperature": 1.0,      # ä¿æŒåˆ›é€ æ€§
    "top_p": 0.95,           # ä¿æŒå¤šæ ·æ€§
    "max_tokens": 128000,    # å……åˆ†åˆ©ç”¨æ¨¡å‹é•¿åº¦
    "presence_penalty": 0.1, # è½»å¾®æƒ©ç½šé‡å¤
    "frequency_penalty": 0.1,
}
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [vLLMå®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [OpenAI APIå…¼å®¹æ¥å£](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)
- [DeepSeekMath-V2æ¨¡å‹](https://huggingface.co/deepseek-ai/deepseek-math-v2)

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. vLLMæœåŠ¡æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯
2. ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´ä¸‹è½½
3. éªŒè¯GPUé©±åŠ¨å’ŒCUDAç‰ˆæœ¬å…¼å®¹æ€§
4. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç«¯å£å ç”¨æƒ…å†µ