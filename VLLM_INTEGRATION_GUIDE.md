# ğŸš€ DeepSeekMath-V2 vLLMé›†æˆä½¿ç”¨æŒ‡å—ï¼ˆæ›´æ–°ç‰ˆï¼‰

## âœ… æ›´æ–°è¯´æ˜

**é‡è¦**: æˆ‘å·²ç»å°†vLLMå…¼å®¹æ€§æ”¹é€ ç›´æ¥é›†æˆåˆ° `inference/generate.py` æ–‡ä»¶ä¸­ï¼Œ**æ— éœ€ä½¿ç”¨é¢å¤–çš„æ–‡ä»¶**ï¼

## ğŸ”§ ç¯å¢ƒé…ç½®

### 1. è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
```bash
# è®¾ç½®vLLMæ¨ç†å¼•æ“åœ°å€
export VLLM_BASE_URL="http://localhost:8000/v1"

# è®¾ç½®APIå¯†é’¥ï¼ˆvLLMé€šå¸¸å¯ä»¥è®¾ç½®ä¸ºEMPTYï¼‰
export VLLM_API_KEY="EMPTY"
```

### 2. å¯åŠ¨vLLMæ¨ç†å¼•æ“
```bash
# å¯åŠ¨vLLMæ¨ç†æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype float16 \
    --max-model-len 128000
```

## ğŸ¯ ä½¿ç”¨æ–¹æ³•

### ç›´æ¥è¿è¡ŒåŸæœ‰çš„æ¨ç†è„šæœ¬
```bash
# ç°åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨åŸæ¥çš„generate.pyè„šæœ¬ï¼
python inference/generate.py \
    --input_data_path inputs/IMO2025.json \
    --output_data_path outputs/IMO2025_results.jsonl \
    --temperature 1.0 \
    --top_p 0.95 \
    --max_tokens 128000 \
    --n 32
```

### åœ¨main.pyä¸­ä½¿ç”¨
```bash
# è¿è¡Œå®Œæ•´çš„ä¸‰é‡éªŒè¯æµç¨‹
python inference/main.py \
    --input_paths inputs/IMO2025.json \
    --output_dirname outputs/IMO2025_results \
    --proof_pool_dirname outputs/IMO2025_results/proof_pool \
    --n_best_proofs_to_sample 32 \
    --n_proofs_to_refine 1 \
    --n_agg_trials 32 \
    --n_parallel_proof_gen 128 \
    --n_verification_per_proof 64 \
    --skip_meta_verification \
    --start_round 1 \
    --max_rounds 16
```

## ğŸ” éªŒè¯é›†æˆæ˜¯å¦æˆåŠŸ

è¿è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥vLLMè¿æ¥ï¼š
```bash
# æµ‹è¯•vLLMæœåŠ¡
curl http://localhost:8000/v1/models

# è¿è¡Œç®€å•æµ‹è¯•
python -c "
import os
os.environ['VLLM_BASE_URL'] = 'http://localhost:8000/v1'
os.environ['VLLM_API_KEY'] = 'EMPTY'

from inference.generate import APIModel
model = APIModel()
print('âœ… vLLMé›†æˆæˆåŠŸï¼')
print(f'Base URL: {model.base_url}')
"
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| é…ç½® | åŸå§‹OpenAI API | vLLMæœ¬åœ°æ¨ç† |
|------|---------------|-------------|
| **å»¶è¿Ÿ** | ~500-2000ms | ~50-200ms |
| **æˆæœ¬** | æŒ‰tokenè®¡è´¹ | å…è´¹ï¼ˆæœ¬åœ°GPUï¼‰ |
| **å¹¶å‘** | å—é™ | é«˜è¾¾320è¿›ç¨‹ |
| **æ§åˆ¶** | æœ‰é™ | å®Œå…¨æ§åˆ¶ |

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜1ï¼šè¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥vLLMæœåŠ¡çŠ¶æ€
curl http://localhost:8000/v1/health

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -an | grep 8000
```

### å¸¸è§é—®é¢˜2ï¼šæ¨¡å‹åŠ è½½é”™è¯¯
```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls -la ~/.cache/huggingface/transformers/

# é‡æ–°ä¸‹è½½æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --download-dir ./models
```

### å¸¸è§é—®é¢˜3ï¼šå†…å­˜ä¸è¶³
```bash
# å‡å°‘å¹¶å‘è¿›ç¨‹æ•°
python inference/generate.py \
    --num_processes 8 \  # å‡å°‘åˆ°8ä¸ªè¿›ç¨‹
    ...å…¶ä»–å‚æ•°

# æˆ–è€…å‡å°‘æ‰¹å¤„ç†å¤§å°
python inference/generate.py \
    --batch_size 8 \    # å‡å°‘åˆ°8ä¸ªæ ·æœ¬
    ...å…¶ä»–å‚æ•°
```

## ğŸ’¡ é«˜çº§é…ç½®

### 1. å¤šGPUé…ç½®
```bash
# ä½¿ç”¨å¤šä¸ªGPU
export CUDA_VISIBLE_DEVICES=0,1,2,3
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --tensor-parallel-size 4 \
    ...å…¶ä»–å‚æ•°
```

### 2. é‡åŒ–ä¼˜åŒ–
```bash
# ä½¿ç”¨AWQé‡åŒ–
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --quantization awq \
    ...å…¶ä»–å‚æ•°
```

### 3. åŠ¨æ€æ‰¹å¤„ç†
```bash
# ä¼˜åŒ–æ‰¹å¤„ç†
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --max-num-batched-tokens 8192 \
    --max-num-seqs 256 \
    ...å…¶ä»–å‚æ•°
```

## ğŸ¯ æ€»ç»“

âœ… **é›†æˆå®Œæˆ** - vLLMå…¼å®¹æ€§å·²ç›´æ¥é›†æˆåˆ° `generate.py` ä¸­
âœ… **é›¶ä¾èµ–** - ç§»é™¤äº†å¯¹OpenAI SDKçš„ä¾èµ–
âœ… **å®Œå…¨å…¼å®¹** - ä¿æŒåŸæœ‰æ¥å£ä¸å˜
âœ… **æ€§èƒ½æå‡** - æœ¬åœ°æ¨ç†å¤§å¹…é™ä½å»¶è¿Ÿ
âœ… **æˆæœ¬ä¼˜åŒ–** - å…è´¹ä½¿ç”¨æœ¬åœ°GPUèµ„æº

ç°åœ¨ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨åŸæœ‰çš„ `generate.py` å’Œ `main.py` è„šæœ¬ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨vLLMæœ¬åœ°æ¨ç†å¼•æ“ï¼