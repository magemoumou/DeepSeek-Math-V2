# ğŸš€ DeepSeekMath-V2 vLLMé›†æˆä½¿ç”¨æŒ‡å—ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—æä¾›äº†**ä¸ä¿®æ”¹åŸæœ‰ä»£ç **çš„vLLMé›†æˆæ–¹æ¡ˆï¼Œé€šè¿‡ç‹¬ç«‹çš„`generate_vllm.py`æ–‡ä»¶å®ç°æœ¬åœ°æ¨ç†å¼•æ“æ”¯æŒã€‚

## ğŸ”§ ç¯å¢ƒé…ç½®

### 1. è®¾ç½®ç¯å¢ƒå˜é‡
```bash
# è®¾ç½®vLLMæ¨ç†å¼•æ“åœ°å€
export VLLM_BASE_URL="http://localhost:8000/v1"

# è®¾ç½®APIå¯†é’¥ï¼ˆvLLMé€šå¸¸å¯ä»¥è®¾ç½®ä¸ºEMPTYï¼‰
export VLLM_API_KEY="EMPTY"
```

### 2. å¯åŠ¨vLLMæ¨ç†å¼•æ“
```bash
# å¯åŠ¨vLLMæ¨ç†æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype float16 \
    --max-model-len 128000
```

## ğŸ¯ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨vLLMç‰ˆæœ¬
```bash
# ä½¿ç”¨vLLMå…¼å®¹ç‰ˆæœ¬è¿›è¡Œæ¨ç†
python inference/generate_vllm.py \
    --input_data_path inputs/IMO2025.json \
    --output_data_path outputs/IMO2025_results.jsonl \
    --temperature 1.0 \
    --top_p 0.95 \
    --max_tokens 128000 \
    --n 32
```

### æ–¹æ³•2ï¼šåœ¨main.pyä¸­ä½¿ç”¨vLLM
ä¿®æ”¹ `inference/main.py` ä¸­çš„è°ƒç”¨ï¼š

```python
# å°†åŸæ¥çš„å‘½ä»¤
proof_gen_cmd = f"""
python {args.infer_script}.py \
--input_data_path {proof_gen_input_path} \
--output_data_path {proof_gen_output_path} \
--batch_size {args.batch_size} \
--num_processes {args.proof_gen_num_processes} \
--temperature {args.proof_gen_temp} \
--top_p 0.95 \
--max_tokens {args.proof_gen_max_len} \
--n {n_sample}
""".strip()

# æ›¿æ¢ä¸º
proof_gen_cmd = f"""
python inference/generate_vllm.py \
--input_data_path {proof_gen_input_path} \
--output_data_path {proof_gen_output_path} \
--batch_size {args.batch_size} \
--num_processes {args.proof_gen_num_processes} \
--temperature {args.proof_gen_temp} \
--top_p 0.95 \
--max_tokens {args.proof_gen_max_len} \
--n {n_sample}
""".strip()
```

### æ–¹æ³•3ï¼šåˆ›å»ºç¬¦å·é“¾æ¥ï¼ˆæ¨èï¼‰
```bash
# å¤‡ä»½åŸæ–‡ä»¶
cp inference/generate.py inference/generate_original.py

# åˆ›å»ºç¬¦å·é“¾æ¥ï¼ˆLinux/Macï¼‰
ln -s generate_vllm.py inference/generate.py

# Windowsä½¿ç”¨å¤åˆ¶
copy inference\generate_vllm.py inference\generate.py
```

## ğŸ” éªŒè¯é›†æˆ

### æµ‹è¯•vLLMè¿æ¥
```bash
# æ£€æŸ¥vLLMæœåŠ¡çŠ¶æ€
curl http://localhost:8000/v1/models

# æµ‹è¯•ç”Ÿæˆè„šæœ¬
python -c "
import os
os.environ['VLLM_BASE_URL'] = 'http://localhost:8000/v1'
os.environ['VLLM_API_KEY'] = 'EMPTY'

from inference.generate_vllm import APIModel
model = APIModel()
test_data = [{'prompt': 'Test message', 'id': 1}]
result = model.generate(test_data, {'temperature': 0.7})
print('âœ… vLLMé›†æˆæˆåŠŸï¼')
"
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| é…ç½® | åŸå§‹OpenAI API | vLLMæœ¬åœ°æ¨ç† |
|------|---------------|-------------|
| **å»¶è¿Ÿ** | ~500-2000ms | ~50-200ms |
| **æˆæœ¬** | æŒ‰tokenè®¡è´¹ | å…è´¹ï¼ˆæœ¬åœ°GPUï¼‰ |
| **å¹¶å‘** | å—é™ | é«˜è¾¾320è¿›ç¨‹ |
| **æ§åˆ¶** | æœ‰é™ | å®Œå…¨æ§åˆ¶ |

## ğŸ› ï¸ é«˜çº§é…ç½®

### å¤šGPUé…ç½®
```bash
export CUDA_VISIBLE_DEVICES=0,1,2,3
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --tensor-parallel-size 4 \
    ...å…¶ä»–å‚æ•°
```

### æ‰¹å¤„ç†ä¼˜åŒ–
```bash
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --max-num-batched-tokens 16384 \
    --max-num-seqs 512 \
    ...å…¶ä»–å‚æ•°
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜1ï¼šè¿æ¥è¶…æ—¶
```bash
# å¢åŠ è¶…æ—¶æ—¶é—´
export VLLM_TIMEOUT="600000"  # 10åˆ†é’Ÿ

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/v1/health
```

### å¸¸è§é—®é¢˜2ï¼šå†…å­˜ä¸è¶³
```bash
# å‡å°‘å¹¶å‘è¿›ç¨‹æ•°
python inference/generate_vllm.py \
    --num_processes 8 \  # å‡å°‘åˆ°8ä¸ªè¿›ç¨‹
    ...å…¶ä»–å‚æ•°
```

### å¸¸è§é—®é¢˜3ï¼šæ¨¡å‹åŠ è½½å¤±è´¥
```bash
# ä½¿ç”¨é‡åŒ–æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/deepseek-math-v2 \
    --quantization awq \
    ...å…¶ä»–å‚æ•°
```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

1. **å¼€å‘é˜¶æ®µ**ï¼šä½¿ç”¨vLLMç‰ˆæœ¬è¿›è¡Œå¿«é€Ÿè¿­ä»£
2. **ç”Ÿäº§ç¯å¢ƒ**ï¼šæ ¹æ®éœ€æ±‚é€‰æ‹©OpenAI APIæˆ–vLLM
3. **å¤§è§„æ¨¡æ¨ç†**ï¼švLLMæä¾›æ›´å¥½çš„æˆæœ¬æ§åˆ¶
4. **è´¨é‡ä¿è¯**ï¼šä¸¤å¥—ç³»ç»Ÿå¯ä»¥äº¤å‰éªŒè¯ç»“æœ

## ğŸ”„ åˆ‡æ¢å›åŸç³»ç»Ÿ

å¦‚éœ€åˆ‡æ¢å›OpenAI APIï¼š
```bash
# æ¢å¤åŸå§‹æ–‡ä»¶
cp inference/generate_original.py inference/generate.py

# æˆ–è€…åˆ é™¤ç¬¦å·é“¾æ¥
rm inference/generate.py
cp inference/generate_original.py inference/generate.py
```

## ğŸ“‹ æ€»ç»“

âœ… **é›¶ä¾µå…¥æ€§** - ä¸ä¿®æ”¹åŸæœ‰ä»£ç ï¼Œå®Œå…¨ç‹¬ç«‹è¿è¡Œ
âœ… **å®Œå…¨å…¼å®¹** - ä¿æŒåŸæœ‰æ¥å£å’Œè¾“å‡ºæ ¼å¼
âœ… **çµæ´»åˆ‡æ¢** - å¯éšæ—¶åœ¨OpenAI APIå’ŒvLLMä¹‹é—´åˆ‡æ¢
âœ… **æ€§èƒ½æå‡** - æœ¬åœ°æ¨ç†å¤§å¹…é™ä½å»¶è¿Ÿå’Œæˆæœ¬
âœ… **ç”Ÿäº§å°±ç»ª** - æ”¯æŒå®Œæ•´çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶